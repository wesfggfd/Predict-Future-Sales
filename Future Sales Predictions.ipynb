{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8587,"databundleVersionId":868304,"sourceType":"competition"}],"dockerImageVersionId":30260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello again :D, This is another try in Kaggle, This one however is a bit more advanced than usual after checking out the data sets as well as the predictions needed. ","metadata":{}},{"cell_type":"markdown","source":"<font size=\"5\">**Data loading and preprocessing, utility function definition**</font>","metadata":{}},{"cell_type":"code","source":"import gc\nimport itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:39:33.341432Z","iopub.execute_input":"2022-10-10T19:39:33.341969Z","iopub.status.idle":"2022-10-10T19:39:33.938636Z","shell.execute_reply.started":"2022-10-10T19:39:33.341859Z","shell.execute_reply":"2022-10-10T19:39:33.937239Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reduce_mem_usage(df, silent=True, allow_categorical=True, float_dtype=\"float32\"):\n    \"\"\" \n    Iterates through all the columns of a dataframe and downcasts the data type\n     to reduce memory usage. Can also factorize categorical columns to integer dtype.\n    \"\"\"\n    def _downcast_numeric(series, allow_categorical=allow_categorical):\n        \"\"\"\n        Downcast a numeric series into either the smallest possible int dtype or a specified float dtype.\n        \"\"\"\n        if pd.api.types.is_sparse(series.dtype) is True:\n            return series\n        elif pd.api.types.is_numeric_dtype(series.dtype) is False:\n            if pd.api.types.is_datetime64_any_dtype(series.dtype):\n                return series\n            else:\n                if allow_categorical:\n                    return series\n                else:\n                    codes, uniques = series.factorize()\n                    series = pd.Series(data=codes, index=series.index)\n                    series = _downcast_numeric(series)\n                    return series\n        else:\n            series = pd.to_numeric(series, downcast=\"integer\")\n        if pd.api.types.is_float_dtype(series.dtype):\n            series = series.astype(float_dtype)\n        return series\n\n    if silent is False:\n        start_mem = np.sum(df.memory_usage()) / 1024 ** 2\n        print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n    if df.ndim == 1:\n        df = _downcast_numeric(df)\n    else:\n        for col in df.columns:\n            df.loc[:, col] = _downcast_numeric(df.loc[:,col])\n    if silent is False:\n        end_mem = np.sum(df.memory_usage()) / 1024 ** 2\n        print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n        print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n\n\ndef shrink_mem_new_cols(matrix, oldcols=None, allow_categorical=False):\n    # Calls reduce_mem_usage on columns which have not yet been optimized\n    if oldcols is not None:\n        newcols = matrix.columns.difference(oldcols)\n    else:\n        newcols = matrix.columns\n    matrix.loc[:,newcols] = reduce_mem_usage(matrix.loc[:,newcols], allow_categorical=allow_categorical)\n    oldcols = matrix.columns  # This is used to track which columns have already been downcast\n    return matrix, oldcols\n\n\ndef list_if_not(s, dtype=str):\n    # Puts a variable in a list if it is not already a list\n    if type(s) not in (dtype, list):\n        raise TypeError\n    if (s != \"\") & (type(s) is not list):\n        s = [s]\n    return s","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:39:33.940857Z","iopub.execute_input":"2022-10-10T19:39:33.941221Z","iopub.status.idle":"2022-10-10T19:39:33.956335Z","shell.execute_reply.started":"2022-10-10T19:39:33.941189Z","shell.execute_reply":"2022-10-10T19:39:33.955322Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Importing Data","metadata":{}},{"cell_type":"code","source":"items = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\ntrain = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:39:33.957435Z","iopub.execute_input":"2022-10-10T19:39:33.958299Z","iopub.status.idle":"2022-10-10T19:39:36.630269Z","shell.execute_reply.started":"2022-10-10T19:39:33.958245Z","shell.execute_reply":"2022-10-10T19:39:36.62901Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Converting to dtypes ","metadata":{}},{"cell_type":"code","source":"train[\"date\"] = pd.to_datetime(train[\"date\"], format=\"%d.%m.%Y\")","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:39:36.632905Z","iopub.execute_input":"2022-10-10T19:39:36.633288Z","iopub.status.idle":"2022-10-10T19:39:37.098139Z","shell.execute_reply.started":"2022-10-10T19:39:36.633252Z","shell.execute_reply":"2022-10-10T19:39:37.096342Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<font size = 5 ><b>DATA CLEANING</b> </font>","metadata":{}},{"cell_type":"code","source":"## Merge some duplicate shops\ntrain[\"shop_id\"] = train[\"shop_id\"].replace({0: 57, 1: 58, 11: 10, 40: 39})\n# Keep only shops that are in the test set\ntrain = train.loc[train.shop_id.isin(test[\"shop_id\"].unique()), :]\n# Drop training items with extreme or negative prices or sales counts\ntrain = train[(train[\"item_price\"] > 0) & (train[\"item_price\"] < 50000)]\ntrain = train[(train[\"item_cnt_day\"] > 0) & (train[\"item_cnt_day\"] < 1000)]\n","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:39:37.102297Z","iopub.execute_input":"2022-10-10T19:39:37.103011Z","iopub.status.idle":"2022-10-10T19:39:37.529879Z","shell.execute_reply.started":"2022-10-10T19:39:37.102964Z","shell.execute_reply":"2022-10-10T19:39:37.528543Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The test data is every combination of shops and items that had a sale in the test month,the target as the total month's sales made for each of these shop-item combinations. I created a  training matrix that is made to replicate this structure for every month in the training data period. The test items are concatenated to the end of the training data so that features can be generated for the test period.","metadata":{}},{"cell_type":"code","source":"def create_testlike_train(sales_train, test=None):\n    indexlist = []\n    for i in sales_train.date_block_num.unique():\n        x = itertools.product(\n            [i],\n            sales_train.loc[sales_train.date_block_num == i].shop_id.unique(),\n            sales_train.loc[sales_train.date_block_num == i].item_id.unique(),\n        )\n        indexlist.append(np.array(list(x)))\n    df = pd.DataFrame(\n        data=np.concatenate(indexlist, axis=0),\n        columns=[\"date_block_num\", \"shop_id\", \"item_id\"],\n    )\n\n    # Add revenue column to sales_train\n    sales_train[\"item_revenue_day\"] = sales_train[\"item_price\"] * sales_train[\"item_cnt_day\"]\n    # Aggregate item_id / shop_id item_cnts and revenue at the month level\n    sales_train_grouped = sales_train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n        item_cnt_month=pd.NamedAgg(column=\"item_cnt_day\", aggfunc=\"sum\"),\n        item_revenue_month=pd.NamedAgg(column=\"item_revenue_day\", aggfunc=\"sum\"),\n    )\n\n    # Merge the grouped data with the index\n    df = df.merge(\n        sales_train_grouped, how=\"left\", on=[\"date_block_num\", \"shop_id\", \"item_id\"],\n    )\n\n    if test is not None:\n        test[\"date_block_num\"] = 34\n        test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n        test[\"shop_id\"] = test.shop_id.astype(np.int8)\n        test[\"item_id\"] = test.item_id.astype(np.int16)\n        test = test.drop(columns=\"ID\")\n\n        df = pd.concat([df, test[[\"date_block_num\", \"shop_id\", \"item_id\"]]])\n\n    # Fill empty item_cnt entries with 0\n    df.item_cnt_month = df.item_cnt_month.fillna(0)\n    df.item_revenue_month = df.item_revenue_month.fillna(0)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:39:37.531606Z","iopub.execute_input":"2022-10-10T19:39:37.53265Z","iopub.status.idle":"2022-10-10T19:39:37.545006Z","shell.execute_reply.started":"2022-10-10T19:39:37.532608Z","shell.execute_reply":"2022-10-10T19:39:37.543385Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matrix = create_testlike_train(train, test)\ndel(test)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:39:37.546663Z","iopub.execute_input":"2022-10-10T19:39:37.547036Z","iopub.status.idle":"2022-10-10T19:39:49.896644Z","shell.execute_reply.started":"2022-10-10T19:39:37.547004Z","shell.execute_reply":"2022-10-10T19:39:49.895252Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We need to reduce memory usage through the function previously created, preventing memory overflows & leaks through errors in the kaggle notebook","metadata":{}},{"cell_type":"code","source":"matrix = reduce_mem_usage(matrix, silent=False)\noldcols = matrix.columns","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:39:49.898316Z","iopub.execute_input":"2022-10-10T19:39:49.89873Z","iopub.status.idle":"2022-10-10T19:39:51.563636Z","shell.execute_reply.started":"2022-10-10T19:39:49.898693Z","shell.execute_reply":"2022-10-10T19:39:51.562393Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"items.query(\"item_id>3564\").head(5)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:39:51.56518Z","iopub.execute_input":"2022-10-10T19:39:51.565794Z","iopub.status.idle":"2022-10-10T19:39:51.588695Z","shell.execute_reply.started":"2022-10-10T19:39:51.565754Z","shell.execute_reply":"2022-10-10T19:39:51.58735Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I noticed that items in the table are ordered alphabetically according to the item_name field, So similar items are listed together. We can use this ordering to group together related items ","metadata":{}},{"cell_type":"markdown","source":"We can run measure similarities between items and group them together, using a py package called fuzzywuzzy, and asign items to a group if their match value are above a certain threshold.","metadata":{}},{"cell_type":"code","source":"import re\n\nfrom fuzzywuzzy import fuzz\n\n\ndef add_item_name_groups(matrix, train, items, sim_thresh, feature_name=\"item_name_group\"):\n    def partialmatchgroups(items, sim_thresh=sim_thresh):\n        def strip_brackets(string):\n            string = re.sub(r\"\\(.*?\\)\", \"\", string)\n            string = re.sub(r\"\\[.*?\\]\", \"\", string)\n            return string\n\n        items = items.copy()\n        items[\"nc\"] = items.item_name.apply(strip_brackets)\n        items[\"ncnext\"] = np.concatenate((items[\"nc\"].to_numpy()[1:], np.array([\"\"])))\n\n        def partialcompare(s):\n            return fuzz.partial_ratio(s[\"nc\"], s[\"ncnext\"])\n\n        items[\"partialmatch\"] = items.apply(partialcompare, axis=1)\n        # Assign groups\n        grp = 0\n        for i in range(items.shape[0]):\n            items.loc[i, \"partialmatchgroup\"] = grp\n            if items.loc[i, \"partialmatch\"] < sim_thresh:\n                grp += 1\n        items = items.drop(columns=[\"nc\", \"ncnext\", \"partialmatch\"])\n        return items\n\n    items = partialmatchgroups(items)\n    items = items.rename(columns={\"partialmatchgroup\": feature_name})\n    items = items.drop(columns=\"partialmatchgroup\", errors=\"ignore\")\n\n    items[feature_name] = items[feature_name].apply(str)\n    items[feature_name] = items[feature_name].factorize()[0]\n    matrix = matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n    train = train.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n    return matrix, train\n\n\nmatrix, train = add_item_name_groups(matrix, train, items, 65)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:39:51.594859Z","iopub.execute_input":"2022-10-10T19:39:51.595841Z","iopub.status.idle":"2022-10-10T19:40:02.018732Z","shell.execute_reply.started":"2022-10-10T19:39:51.595797Z","shell.execute_reply":"2022-10-10T19:40:02.017244Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are certain music items that would be quite difficult to use the last function on, you know, to put into similar groups. The idea is to assign music items into groups according to the artist.\n3 common patterns exist to indicate the artist's name which are:\n-His name is all in uppercase,seperated by dot & a space.\nFor non-musical stuff, the items will be grouped up according to the first word.","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\n\ndef add_first_word_features(matrix, items=items, feature_name=\"artist_name_or_first_word\"):\n    # This extracts artist names for music categories and adds them as a feature.\n    def extract_artist(st):\n        st = st.strip()\n        if st.startswith(\"V/A\"):\n            artist = \"V/A\"\n        elif st.startswith(\"СБ\"):\n            artist = \"СБ\"\n        else:\n            # Retrieves artist names using the double space or all uppercase pattern\n            mus_artist_dubspace = re.compile(r\".{2,}?(?=\\s{2,})\")\n            match_dubspace = mus_artist_dubspace.match(st)\n            mus_artist_capsonly = re.compile(r\"^([^a-zа-я]+\\s)+\")\n            match_capsonly = mus_artist_capsonly.match(st)\n            candidates = [match_dubspace, match_capsonly]\n            candidates = [m[0] for m in candidates if m is not None]\n            # Sometimes one of the patterns catches some extra words so choose the shortest one\n            if len(candidates):\n                artist = min(candidates, key=len)\n            else:\n                # If neither of the previous patterns found something, use the dot-space pattern\n                mus_artist_dotspace = re.compile(r\".{2,}?(?=\\.\\s)\")\n                match = mus_artist_dotspace.match(st)\n                if match:\n                    artist = match[0]\n                else:\n                    artist = \"\"\n        artist = artist.upper()\n        artist = re.sub(r\"[^A-ZА-Я ]||\\bTHE\\b\", \"\", artist)\n        artist = re.sub(r\"\\s{2,}\", \" \", artist)\n        artist = artist.strip()\n        return artist\n\n    items = items.copy()\n    all_stopwords = stopwords.words(\"russian\")\n    all_stopwords = all_stopwords + stopwords.words(\"english\")\n\n    def first_word(string):\n        # This cleans the string of special characters, excess spaces and stopwords then extracts the first word\n        string = re.sub(r\"[^\\w\\s]\", \"\", string)\n        string = re.sub(r\"\\s{2,}\", \" \", string)\n        tokens = string.lower().split()\n        tokens = [t for t in tokens if t not in all_stopwords]\n        token = tokens[0] if len(tokens) > 0 else \"\"\n        return token\n\n    music_categories = [55, 56, 57, 58, 59, 60]\n    items.loc[items.item_category_id.isin(music_categories), feature_name] = items.loc[\n        items.item_category_id.isin(music_categories), \"item_name\"\n    ].apply(extract_artist)\n    items.loc[items[feature_name] == \"\", feature_name] = \"other music\"\n    items.loc[~items.item_category_id.isin(music_categories), feature_name] = items.loc[\n        ~items.item_category_id.isin(music_categories), \"item_name\"\n    ].apply(first_word)\n    items.loc[items[feature_name] == \"\", feature_name] = \"other non-music\"\n    items[feature_name] = items[feature_name].factorize()[0]\n    matrix = matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\",)\n    return matrix\n\n\nmatrix = add_first_word_features(\n    matrix, items=items, feature_name=\"artist_name_or_first_word\"\n)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:40:02.023315Z","iopub.execute_input":"2022-10-10T19:40:02.023746Z","iopub.status.idle":"2022-10-10T19:40:04.968787Z","shell.execute_reply.started":"2022-10-10T19:40:02.023706Z","shell.execute_reply":"2022-10-10T19:40:04.967489Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Similarities also exists in the item_name field, the most notable one is their length mostly because similar items tend to have the same length.\nWe're gonna use that as a feature.","metadata":{}},{"cell_type":"code","source":"import re\ndef clean_item_name(string):\n    # Removes bracketed terms, special characters and extra whitespace\n    string = re.sub(r\"\\[.*?\\]\", \"\", string)\n    string = re.sub(r\"\\(.*?\\)\", \"\", string)\n    string = re.sub(r\"[^A-ZА-Яa-zа-я0-9 ]\", \"\", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    string = string.lower()\n    return string\n\nitems[\"item_name_cleaned_length\"] = items[\"item_name\"].apply(clean_item_name).apply(len)\nitems[\"item_name_length\"] = items[\"item_name\"].apply(len)\nmatrix = matrix.merge(items[['item_id', 'item_name_length', 'item_name_cleaned_length']], how='left', on='item_id')\nitems = items.drop(columns=['item_name_length', 'item_name_cleaned_length'])","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:40:04.97012Z","iopub.execute_input":"2022-10-10T19:40:04.970447Z","iopub.status.idle":"2022-10-10T19:40:06.97037Z","shell.execute_reply.started":"2022-10-10T19:40:04.970417Z","shell.execute_reply":"2022-10-10T19:40:06.96928Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Created name features\")\nmatrix, oldcols = shrink_mem_new_cols(matrix, oldcols)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:40:06.972096Z","iopub.execute_input":"2022-10-10T19:40:06.972446Z","iopub.status.idle":"2022-10-10T19:40:08.883964Z","shell.execute_reply.started":"2022-10-10T19:40:06.972414Z","shell.execute_reply":"2022-10-10T19:40:08.882994Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Time of sales are also important\nWe can use the number of days since the first & last sale of each item to make a sales-per-day feature(\"item_cnt_day_avg\"). This way we can use that function to correct sales count for items that haven't reached a month old so their count is faulty.","metadata":{}},{"cell_type":"code","source":"def add_time_features(m, train, correct_item_cnt_day=False):\n    from pandas.tseries.offsets import Day, MonthBegin, MonthEnd\n\n    def item_shop_age_months(m):\n        m[\"item_age\"] = m.groupby(\"item_id\")[\"date_block_num\"].transform(\n            lambda x: x - x.min()\n        )\n        # Sales tend to plateau after 12 months\n        m[\"new_item\"] = m[\"item_age\"] == 0\n        m[\"new_item\"] = m[\"new_item\"].astype(\"int8\")\n        m[\"shop_age\"] = (\n            m.groupby(\"shop_id\")[\"date_block_num\"]\n            .transform(lambda x: x - x.min())\n            .astype(\"int8\")\n        )\n        return m\n\n    # Add dummy values for the test month so that features are created correctly\n    dummies = m.loc[m.date_block_num == 34, [\"date_block_num\", \"shop_id\", \"item_id\"]]\n    dummies = dummies.assign(\n        date=pd.to_datetime(\"2015-11-30\"), item_price=1, item_cnt_day=0, item_revenue_day=0,\n    )\n    train = pd.concat([train, dummies])\n    del dummies\n\n    month_last_day = train.groupby(\"date_block_num\").date.max().rename(\"month_last_day\")\n    month_last_day[~month_last_day.dt.is_month_end] = (\n        month_last_day[~month_last_day.dt.is_month_end] + MonthEnd()\n    )\n    month_first_day = train.groupby(\"date_block_num\").date.min().rename(\"month_first_day\")\n    month_first_day[~month_first_day.dt.is_month_start] = (\n        month_first_day[~month_first_day.dt.is_month_start] - MonthBegin()\n    )\n    month_length = (month_last_day - month_first_day + Day()).rename(\"month_length\")\n    first_shop_date = train.groupby(\"shop_id\").date.min().rename(\"first_shop_date\")\n    first_item_date = train.groupby(\"item_id\").date.min().rename(\"first_item_date\")\n    first_shop_item_date = (\n        train.groupby([\"shop_id\", \"item_id\"]).date.min().rename(\"first_shop_item_date\")\n    )\n    first_item_name_group_date = (\n        train.groupby(\"item_name_group\").date.min().rename(\"first_name_group_date\")\n    )\n\n    m = m.merge(month_first_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n    m = m.merge(month_last_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n    m = m.merge(month_length, left_on=\"date_block_num\", right_index=True, how=\"left\")\n    m = m.merge(first_shop_date, left_on=\"shop_id\", right_index=True, how=\"left\")\n    m = m.merge(first_item_date, left_on=\"item_id\", right_index=True, how=\"left\")\n    m = m.merge(\n        first_shop_item_date, left_on=[\"shop_id\", \"item_id\"], right_index=True, how=\"left\"\n    )\n    m = m.merge(\n        first_item_name_group_date, left_on=\"item_name_group\", right_index=True, how=\"left\"\n    )\n\n    # Calculate how long the item was sold for in each month and use this to calculate average sales per day\n    m[\"shop_open_days\"] = m[\"month_last_day\"] - m[\"first_shop_date\"] + Day()\n    m[\"item_first_sale_days\"] = m[\"month_last_day\"] - m[\"first_item_date\"] + Day()\n    m[\"item_in_shop_days\"] = (\n        m[[\"shop_open_days\", \"item_first_sale_days\", \"month_length\"]].min(axis=1).dt.days\n    )\n    m = m.drop(columns=\"item_first_sale_days\")\n    m[\"item_cnt_day_avg\"] = m[\"item_cnt_month\"] / m[\"item_in_shop_days\"]\n    m[\"month_length\"] = m[\"month_length\"].dt.days\n\n    # Calculate the time differences from the beginning of the month so they can be used as features without lagging\n    m[\"shop_open_days\"] = m[\"month_first_day\"] - m[\"first_shop_date\"]\n    m[\"first_item_sale_days\"] = m[\"month_first_day\"] - m[\"first_item_date\"]\n    m[\"first_shop_item_sale_days\"] = m[\"month_first_day\"] - m[\"first_shop_item_date\"]\n    m[\"first_name_group_sale_days\"] = m[\"month_first_day\"] - m[\"first_name_group_date\"]\n    m[\"shop_open_days\"] = m[\"shop_open_days\"].dt.days.fillna(0).clip(lower=0)\n    m[\"first_item_sale_days\"] = (\n        m[\"first_item_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999)\n    )\n    m[\"first_shop_item_sale_days\"] = (\n        m[\"first_shop_item_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999)\n    )\n    m[\"first_name_group_sale_days\"] = (\n        m[\"first_name_group_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999)\n    )\n\n    # Add days since last sale\n    def last_sale_days(matrix):\n        last_shop_item_dates = []\n        for dbn in range(1, 35):\n            lsid_temp = (\n                train.query(f\"date_block_num<{dbn}\")\n                .groupby([\"shop_id\", \"item_id\"])\n                .date.max()\n                .rename(\"last_shop_item_sale_date\")\n                .reset_index()\n            )\n            lsid_temp[\"date_block_num\"] = dbn\n            last_shop_item_dates.append(lsid_temp)\n\n        last_shop_item_dates = pd.concat(last_shop_item_dates)\n        matrix = matrix.merge(\n            last_shop_item_dates, on=[\"date_block_num\", \"shop_id\", \"item_id\"], how=\"left\"\n        )\n\n        def days_since_last_feat(m, feat_name, date_feat_name, missingval):\n            m[feat_name] = (m[\"month_first_day\"] - m[date_feat_name]).dt.days\n            m.loc[m[feat_name] > 2000, feat_name] = missingval\n            m.loc[m[feat_name].isna(), feat_name] = missingval\n            return m\n\n        matrix = days_since_last_feat(\n            matrix, \"last_shop_item_sale_days\", \"last_shop_item_sale_date\", 9999\n        )\n\n        matrix = matrix.drop(columns=[\"last_shop_item_sale_date\"])\n        return matrix\n\n    m = last_sale_days(m)\n    # Month id feature\n    m[\"month\"] = m[\"month_first_day\"].dt.month\n\n    m = m.drop(\n        columns=[\n            \"first_day\",\n            \"month_first_day\",\n            \"month_last_day\",\n            \"first_shop_date\",\n            \"first_item_date\",\n            \"first_name_group_date\",\n            \"item_in_shop_days\",\n            \"first_shop_item_date\",\n            \"month_length\",\n        ],\n        errors=\"ignore\",\n    )\n\n    m = item_shop_age_months(m)\n\n    if correct_item_cnt_day == True:\n        m[\"item_cnt_month_original\"] = m[\"item_cnt_month\"]\n        m[\"item_cnt_month\"] = m[\"item_cnt_day_avg\"] * m[\"month_length\"]\n\n    return m\n\n        ","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:40:08.885725Z","iopub.execute_input":"2022-10-10T19:40:08.886067Z","iopub.status.idle":"2022-10-10T19:40:08.914883Z","shell.execute_reply.started":"2022-10-10T19:40:08.886037Z","shell.execute_reply":"2022-10-10T19:40:08.913758Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matrix = add_time_features(matrix, train, False)\nprint(\"Time features created\")","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:40:08.917003Z","iopub.execute_input":"2022-10-10T19:40:08.917793Z","iopub.status.idle":"2022-10-10T19:40:56.296658Z","shell.execute_reply.started":"2022-10-10T19:40:08.917745Z","shell.execute_reply":"2022-10-10T19:40:56.295364Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Checking price as a feature, what we can use is the price of the item in the last month in which it was sold, and compare it to other items in the same category","metadata":{}},{"cell_type":"code","source":"def add_price_features(matrix, train):\n    # Get mean prices per month from train dataframe\n    price_features = train.groupby([\"date_block_num\", \"item_id\"]).item_price.mean()\n    price_features = pd.DataFrame(price_features)\n    price_features = price_features.reset_index()\n    # Calculate normalized differenced from mean category price per month\n    price_features = price_features.merge(\n        items[[\"item_id\", \"item_category_id\"]], how=\"left\", on=\"item_id\"\n    )\n    price_features[\"norm_diff_cat_price\"] = price_features.groupby(\n        [\"date_block_num\", \"item_category_id\"]\n    )[\"item_price\"].transform(lambda x: (x - x.mean()) / x.mean())\n    # Retain only the necessary features\n    price_features = price_features[\n        [\n            \"date_block_num\",\n            \"item_id\",\n            \"item_price\",\n            \"norm_diff_cat_price\",\n        ]\n    ]\n\n    features = [\n        \"item_price\",\n        \"norm_diff_cat_price\",\n    ]\n    newnames = [\"last_\" + f for f in features]\n    aggs = {f: \"last\" for f in features}\n    renames = {f: \"last_\" + f for f in features}\n    features = []\n    for dbn in range(1, 35):\n        f_temp = (\n            price_features.query(f\"date_block_num<{dbn}\")\n            .groupby(\"item_id\")\n            .agg(aggs)\n            .rename(columns=renames)\n        )\n        f_temp[\"date_block_num\"] = dbn\n        features.append(f_temp)\n    features = pd.concat(features).reset_index()\n    matrix = matrix.merge(features, on=[\"date_block_num\", \"item_id\"], how=\"left\")\n    return matrix","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:40:56.298354Z","iopub.execute_input":"2022-10-10T19:40:56.299008Z","iopub.status.idle":"2022-10-10T19:40:56.311499Z","shell.execute_reply.started":"2022-10-10T19:40:56.298957Z","shell.execute_reply":"2022-10-10T19:40:56.310269Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matrix = add_price_features(matrix, train)\ndel(train)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:40:56.313249Z","iopub.execute_input":"2022-10-10T19:40:56.314424Z","iopub.status.idle":"2022-10-10T19:41:01.175151Z","shell.execute_reply.started":"2022-10-10T19:40:56.314369Z","shell.execute_reply":"2022-10-10T19:41:01.174219Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I'm gonna create a category called supercategory to lighten & order the data a little. This category will add an item and its genre together e.g: supercategory(\"games\",\"music\") and platform(\"PS4\",\"mp3\")","metadata":{}},{"cell_type":"code","source":"matrix = matrix.merge(items[['item_id', 'item_category_id']], on='item_id', how='left')\n\nplatform_map = {\n    0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 8, 10: 1, 11: 2,\n    12: 3, 13: 4, 14: 5, 15: 6, 16: 7, 17: 8, 18: 1, 19: 2, 20: 3, 21: 4, 22: 5,\n    23: 6, 24: 7, 25: 8, 26: 9, 27: 10, 28: 0, 29: 0, 30: 0, 31: 0, 32: 8, 33: 11,\n    34: 11, 35: 3, 36: 0, 37: 12, 38: 12, 39: 12, 40: 13, 41: 13, 42: 14, 43: 15,\n    44: 15, 45: 15, 46: 14, 47: 14, 48: 14, 49: 14, 50: 14, 51: 14, 52: 14, 53: 14,\n    54: 8, 55: 16, 56: 16, 57: 17, 58: 18, 59: 13, 60: 16, 61: 8, 62: 8, 63: 8, 64: 8,\n    65: 8, 66: 8, 67: 8, 68: 8, 69: 8, 70: 8, 71: 8, 72: 8, 73: 0, 74: 10, 75: 0,\n    76: 0, 77: 0, 78: 0, 79: 8, 80: 8, 81: 8, 82: 8, 83: 8,\n}\nmatrix['platform_id'] = matrix['item_category_id'].map(platform_map)\n\nsupercat_map = {\n    0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 2, 9: 2, 10: 1, 11: 1, 12: 1,\n    13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 3, 19: 3, 20: 3, 21: 3, 22: 3, 23: 3,\n    24: 3, 25: 0, 26: 2, 27: 3, 28: 3, 29: 3, 30: 3, 31: 3, 32: 2, 33: 2, 34: 2,\n    35: 2, 36: 2, 37: 4, 38: 4, 39: 4, 40: 4, 41: 4, 42: 5, 43: 5, 44: 5, 45: 5,\n    46: 5, 47: 5, 48: 5, 49: 5, 50: 5, 51: 5, 52: 5, 53: 5, 54: 5, 55: 6, 56: 6,\n    57: 6, 58: 6, 59: 6, 60: 6, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0,\n    68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 7, 74: 7, 75: 7, 76: 7, 77: 7, 78: 7,\n    79: 2, 80: 2, 81: 0, 82: 0, 83: 0\n}\nmatrix['supercategory_id'] = matrix['item_category_id'].map(supercat_map)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:41:01.176794Z","iopub.execute_input":"2022-10-10T19:41:01.177427Z","iopub.status.idle":"2022-10-10T19:41:03.696268Z","shell.execute_reply.started":"2022-10-10T19:41:01.177391Z","shell.execute_reply":"2022-10-10T19:41:03.694908Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The next part I took from https://www.kaggle.com/dlarionov/feature-engineering-xgboost, to categorise shop city :p","metadata":{}},{"cell_type":"code","source":"def add_city_codes(matrix, shops):\n    shops.loc[\n        shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', \"shop_name\"\n    ] = 'СергиевПосад ТЦ \"7Я\"'\n    shops[\"city\"] = shops[\"shop_name\"].str.split(\" \").map(lambda x: x[0])\n    shops.loc[shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\n    shops[\"city_code\"] = shops[\"city\"].factorize()[0]\n    shop_labels = shops[[\"shop_id\", \"city_code\"]]\n    matrix = matrix.merge(shop_labels, on='shop_id', how='left')\n    return matrix\n\nmatrix = add_city_codes(matrix, shops)\ndel(shops)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:41:03.698022Z","iopub.execute_input":"2022-10-10T19:41:03.698367Z","iopub.status.idle":"2022-10-10T19:41:05.962426Z","shell.execute_reply.started":"2022-10-10T19:41:03.698336Z","shell.execute_reply":"2022-10-10T19:41:05.961268Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This model is gonna be a clustering one. \nShop & item categories will be grouped into clusters according to their sales. This next function will perform & plot the results of a PCA decomposition & clustering of the shops & items categories.\n\nWe're gonna need a silhouette score that's gonna be our metric for cluster quality, this will determine the number of clusters.\n\nFor shop & item categories, most the differences occur on a single dimension, that indicates that the differences are mainly magnitude rather than proportional. \n\nThe item component score plots show that the clustering is mainly into a large cluster containing the large majority of items, and a few clusters containing outlier items.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\n\n\ndef cluster_feature(matrix, target_feature, clust_feature, level_feature, n_components=4, n_clusters=5, aggfunc=\"mean\", exclude=None):\n    start_month = 20\n    end_month = 32\n    pt = matrix.query(f\"date_block_num>{start_month} & date_block_num<={end_month}\")\n    if exclude is not None:\n        pt = matrix[~matrix[clust_feature].isin(exclude)]\n    pt = pt.pivot_table(values=target_feature, columns=clust_feature, index=level_feature, fill_value=0, aggfunc=aggfunc)\n    pt = pt.transpose()\n    pca = PCA(n_components=10)\n    components = pca.fit_transform(pt)\n    components = pd.DataFrame(components)\n    # Plot PCA explained variance\n    sns.set_theme()\n    features = list(range(pca.n_components_))\n    fig = plt.figure(figsize=(10,4))\n    ax = fig.add_subplot(121)\n#     ax.bar(features, pca.explained_variance_ratio_, color=\"black\")\n    sns.barplot(x=features, y=pca.explained_variance_ratio_, ax=ax)\n    plt.title(\"Variance by PCA components\")\n    plt.xlabel(\"component\")\n    plt.ylabel(\"explained variance\")\n    plt.xticks(features)\n\n    scorelist = []\n    nrange = range(2, 10)\n    for n in nrange:\n        clusterer = AgglomerativeClustering(n_clusters=n)\n        labels = clusterer.fit_predict(components)\n        silscore = silhouette_score(pt, labels)\n        scorelist.append(silscore)\n    ax = fig.add_subplot(122)\n    sns.lineplot(x=nrange, y=scorelist, ax=ax)\n    plt.title(\"Clustering quality by number of clusters\")\n    plt.xlabel(\"n clusters\")\n    plt.ylabel(\"silhouette score\")\n\n    pca = PCA(n_components=n_components)\n    components = pca.fit_transform(pt)\n    components = pd.DataFrame(components)\n    clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"average\")\n    labels = clusterer.fit_predict(components)\n    x = components[0]\n    y = components[1]\n    fig = plt.figure(figsize=(10, 4))\n    ax = fig.add_subplot(111)\n    sns.scatterplot(x=x, y=y, hue=labels, palette=sns.color_palette(\"hls\", n_clusters), ax=ax)\n    plt.title(\"Items by cluster\")\n    plt.xlabel(\"component 1 score\")\n    plt.ylabel(\"component 2 score\")\n    for i, txt in enumerate(pt.index.to_list()):\n        ax.annotate(str(txt), (x[i], y[i]))\n    groups = {}\n    for i, s in enumerate(pt.index):\n        groups[s] = labels[i]\n    return groups\n    ","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:41:05.964357Z","iopub.execute_input":"2022-10-10T19:41:05.965262Z","iopub.status.idle":"2022-10-10T19:41:06.112399Z","shell.execute_reply.started":"2022-10-10T19:41:05.965212Z","shell.execute_reply":"2022-10-10T19:41:06.111389Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we've got clustered categories according to their mean sales in each month of the year. \n\nLet's print it out & see what we got","metadata":{}},{"cell_type":"code","source":"category_group_dict = cluster_feature(matrix, 'item_cnt_month', 'item_category_id', 'date_block_num', n_components=2, n_clusters=4, aggfunc=\"mean\", exclude =[])\nmatrix['category_cluster'] = matrix['item_category_id'].map(category_group_dict)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:41:06.114271Z","iopub.execute_input":"2022-10-10T19:41:06.114733Z","iopub.status.idle":"2022-10-10T19:41:10.917667Z","shell.execute_reply.started":"2022-10-10T19:41:06.114684Z","shell.execute_reply":"2022-10-10T19:41:10.916445Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Shops are clustered through the  sum of sales of each item category.\n\nThe principale component plots shows that shops mainly fiffer in the magnitude of their sales, with shop 31 having a high volume of sales. \nShop 12 & 55 sell items online so that explains the values on the plot.\n","metadata":{}},{"cell_type":"code","source":"shop_group_dict = cluster_feature(matrix, 'item_cnt_month', 'shop_id', 'item_category_id', n_components=4, n_clusters=4, aggfunc=\"mean\", exclude=[36])\nshop_group_dict[36] = shop_group_dict[37]  # Shop36 added separately because it only has one month of data\nmatrix['shop_cluster'] = matrix['shop_id'].map(shop_group_dict)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:41:10.919254Z","iopub.execute_input":"2022-10-10T19:41:10.919651Z","iopub.status.idle":"2022-10-10T19:41:15.21752Z","shell.execute_reply.started":"2022-10-10T19:41:10.919618Z","shell.execute_reply":"2022-10-10T19:41:15.21612Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()\nmatrix, oldcols = shrink_mem_new_cols(matrix, oldcols) \n#Using this function periodically to downcast dtypes \n#to save memory.","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:41:15.218984Z","iopub.execute_input":"2022-10-10T19:41:15.219336Z","iopub.status.idle":"2022-10-10T19:41:23.062985Z","shell.execute_reply.started":"2022-10-10T19:41:15.219306Z","shell.execute_reply":"2022-10-10T19:41:23.060975Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The number of unique item feature counts the number of unique items sharing the same value of a grouping feature or set of features as the current item in the current month, e.g. number of new items in the same category.\n\nThis could considered to be a kind of data leakage feature, as the set of items in each month (and therefore the test set) is determined by whether each item recorded a sale or not in the month being predicted, which isn't known in advance.","metadata":{}},{"cell_type":"code","source":"def uniques(matrix, groupers, name, limitation=None):\n    if limitation is not None:\n        s = (\n            matrix.query(limitation)\n            .groupby(groupers)\n            .item_id.nunique()\n            .rename(name)\n            .reset_index()\n        )\n    else:\n        s = matrix.groupby(groupers).item_id.nunique().rename(name).reset_index()\n    matrix = matrix.merge(s, on=groupers, how=\"left\")\n    matrix[name] = matrix[name].fillna(0)\n    return matrix\n\n\nmatrix = uniques(matrix, [\"date_block_num\"], \"unique_items_month\")\n\nmatrix = uniques(matrix, [\"date_block_num\", \"item_name_group\"], \"name_group_unique_month\")\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_category_id\", \"item_name_group\"],\n    \"name_group_cat_unique_month\",\n)\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_name_group\"],\n    \"name_group_new_unique_month\",\n    limitation=\"new_item==True\",\n)\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_category_id\", \"item_name_group\"],\n    \"name_group_new_cat_unique_month\",\n    limitation=\"new_item==True\",\n)\n\nmatrix = uniques(\n    matrix, [\"date_block_num\", \"artist_name_or_first_word\"], \"first_word_unique_month\"\n)\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_category_id\", \"artist_name_or_first_word\"],\n    \"first_word_cat_unique_month\",\n)\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"artist_name_or_first_word\"],\n    \"first_word_new_unique_month\",\n    limitation=\"new_item==True\",\n)\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_category_id\", \"artist_name_or_first_word\"],\n    \"first_word_new_cat_unique_month\",\n    limitation=\"new_item==True\",\n)\n\nmatrix = uniques(matrix, [\"date_block_num\", \"item_category_id\"], \"unique_items_cat\")\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_category_id\"],\n    \"new_items_cat\",\n    limitation=\"new_item==True\",\n)\nmatrix = uniques(matrix, [\"date_block_num\"], \"new_items_month\", limitation=\"new_item==True\")\n\nmatrix[\"cat_items_proportion\"] = matrix[\"unique_items_cat\"] / matrix[\"unique_items_month\"]\nmatrix[\"name_group_new_proportion_month\"] = (\n    matrix[\"name_group_new_unique_month\"] / matrix[\"name_group_unique_month\"]\n)\n\nmatrix = matrix.drop(columns=[\"unique_items_month\", \"name_group_unique_month\"])","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:41:23.065214Z","iopub.execute_input":"2022-10-10T19:41:23.065813Z","iopub.status.idle":"2022-10-10T19:42:35.56998Z","shell.execute_reply.started":"2022-10-10T19:41:23.065759Z","shell.execute_reply":"2022-10-10T19:42:35.56852Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matrix, oldcols= shrink_mem_new_cols(matrix, oldcols)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:42:35.571918Z","iopub.execute_input":"2022-10-10T19:42:35.572446Z","iopub.status.idle":"2022-10-10T19:42:42.009415Z","shell.execute_reply.started":"2022-10-10T19:42:35.572382Z","shell.execute_reply":"2022-10-10T19:42:42.007922Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We're gonna use panda's pct_change method to calculate the proportional change in the mean sales count for the groups in a specific time.","metadata":{}},{"cell_type":"code","source":"def add_pct_change(\n    matrix,\n    group_feats,\n    target=\"item_cnt_month\",\n    aggfunc=\"mean\",\n    periods=1,\n    lag=1,\n    clip_value=None,\n):\n    periods = list_if_not(periods, int)\n    group_feats = list_if_not(group_feats)\n    group_feats_full = [\"date_block_num\"] + group_feats\n    dat = matrix.pivot_table(\n        index=group_feats + [\"date_block_num\"],\n        values=target,\n        aggfunc=aggfunc,\n        fill_value=0,\n        dropna=False,\n    ).astype(\"float32\")\n    for g in group_feats:\n        firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n        dat = dat.merge(firsts, left_on=g, right_index=True, how=\"left\")\n        dat.loc[dat.index.get_level_values(\"date_block_num\") < dat[\"firsts\"], target] = float(\n            \"nan\"\n        )\n        del dat[\"firsts\"]\n    for period in periods:\n        feat_name = \"_\".join(\n            group_feats + [target] + [aggfunc] + [\"delta\"] + [str(period)] + [f\"lag_{lag}\"]\n        )\n        print(f\"Adding feature {feat_name}\")\n        dat = (\n            dat.groupby(group_feats)[target]\n            .transform(lambda x: x.pct_change(periods=period, fill_method=\"pad\"))\n            .rename(feat_name)\n        )\n        if clip_value is not None:\n            dat = dat.clip(lower=-clip_value, upper=clip_value)\n    dat = dat.reset_index()\n    dat[\"date_block_num\"] += lag\n    matrix = matrix.merge(dat, on=[\"date_block_num\"] + group_feats, how=\"left\")\n    matrix[feat_name] = reduce_mem_usage(matrix[feat_name])\n    return matrix","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:42:42.011255Z","iopub.execute_input":"2022-10-10T19:42:42.01293Z","iopub.status.idle":"2022-10-10T19:42:42.026651Z","shell.execute_reply.started":"2022-10-10T19:42:42.012876Z","shell.execute_reply":"2022-10-10T19:42:42.025556Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matrix = add_pct_change(matrix, [\"item_id\"], \"item_cnt_month\", clip_value=3)\nmatrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", clip_value=3)\nmatrix = add_pct_change(matrix, [\"item_name_group\"], \"item_cnt_month\", clip_value=3)\n#  feature lagged by 12 months, intended to capture seasonal trends\nmatrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", lag=12, clip_value=3,)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:42:42.028106Z","iopub.execute_input":"2022-10-10T19:42:42.028438Z","iopub.status.idle":"2022-10-10T19:43:44.961069Z","shell.execute_reply.started":"2022-10-10T19:42:42.028409Z","shell.execute_reply":"2022-10-10T19:43:44.959415Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matrix, oldcols= shrink_mem_new_cols(matrix, oldcols)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:43:44.967816Z","iopub.execute_input":"2022-10-10T19:43:44.968223Z","iopub.status.idle":"2022-10-10T19:43:45.886744Z","shell.execute_reply.started":"2022-10-10T19:43:44.968187Z","shell.execute_reply":"2022-10-10T19:43:45.885433Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We're gonna create windowed aggregated features which are features aggregated over a specific window to reduce noise. Available windows are expanding (i.e. all preceding timepoints), rolling (i.e. fixed number of equally weighted timepoints) and exponentially weighted mean.\n\nA note about feature names: these are set automatically according to the pattern < grouping features > - < aggregated features > - < monthly aggregation function > - < window type > , where < window type > is either \"rolling - < window aggregation function > - win - < window length in months >\" for square rolling windows, \"expanding - < window aggregation function >\" for expanding windows, and \"ewm_hl - < decay rate in terms of half-life > for exponential weighted means, all connected by underscores.\n","metadata":{}},{"cell_type":"code","source":"shop_id = 16\nitem_id = 482\nim = matrix.query(f\"shop_id=={shop_id} & item_id=={item_id}\")[['date_block_num', 'item_cnt_month']]\nim['moving average'] = im['item_cnt_month'].ewm(halflife=1).mean()\nim['expanding mean'] = im['item_cnt_month'].expanding().mean()\nim['rolling 12 month mean'] = im['item_cnt_month'].rolling(window=12, min_periods=1).mean()\nim = im.set_index('date_block_num')\nax = im.plot(figsize=(12,5), marker='.', title='Time series averaging methods')","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:43:45.888645Z","iopub.execute_input":"2022-10-10T19:43:45.889202Z","iopub.status.idle":"2022-10-10T19:43:46.35398Z","shell.execute_reply.started":"2022-10-10T19:43:45.889166Z","shell.execute_reply":"2022-10-10T19:43:46.352785Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_rolling_stats(\n    matrix,\n    features,\n    window=12,\n    kind=\"rolling\",\n    argfeat=\"item_cnt_month\",\n    aggfunc=\"mean\",\n    rolling_aggfunc=\"mean\",\n    dtype=\"float16\",\n    reshape_source=True,\n    lag_offset=0,\n):\n    def rolling_stat(\n        matrix,\n        source,\n        feats,\n        feat_name,\n        window=12,\n        argfeat=\"item_cnt_month\",\n        aggfunc=\"mean\",\n        dtype=dtype,\n        lag_offset=0,\n    ):\n        # Calculate a statistic on a windowed section of a source table,  grouping on specific features\n        store = []\n        for i in range(2 + lag_offset, 35 + lag_offset):\n            if len(feats) > 0:\n                mes = (\n                    source[source.date_block_num.isin(range(max([i - window, 0]), i))]\n                    .groupby(feats)[argfeat]\n                    .agg(aggfunc)\n                    .astype(dtype)\n                    .rename(feat_name)\n                    .reset_index()\n                )\n            else:\n                mes = {}\n                mes[feat_name] = (\n                    source.loc[\n                        source.date_block_num.isin(range(max([i - window, 0]), i)), argfeat\n                    ]\n                    .agg(aggfunc)\n                    .astype(dtype)\n                )\n                mes = pd.DataFrame(data=mes, index=[i])\n            mes[\"date_block_num\"] = i - lag_offset\n            store.append(mes)\n        store = pd.concat(store)\n        matrix = matrix.merge(store, on=feats + [\"date_block_num\"], how=\"left\")\n        return matrix\n\n    \"\"\" An issue when using windowed functions is that missing values from months when items recorded no sales are skipped rather than being correctly\n    treated as zeroes. Creating a pivot_table fills in the zeros.\"\"\"\n    if (reshape_source == True) or (kind == \"ewm\"):\n        source = matrix.pivot_table(\n            index=features + [\"date_block_num\"],\n            values=argfeat,\n            aggfunc=aggfunc,\n            fill_value=0,\n            dropna=False,\n        ).astype(dtype)\n        for g in features:\n            firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n            source = source.merge(firsts, left_on=g, right_index=True, how=\"left\")\n            # Set values before the items first appearance to nan so they are ignored rather than being treated as zero sales.\n            source.loc[\n                source.index.get_level_values(\"date_block_num\") < source[\"firsts\"], argfeat\n            ] = float(\"nan\")\n            del source[\"firsts\"]\n        source = source.reset_index()\n    else:\n        source = matrix\n\n    if kind == \"rolling\":\n        feat_name = (\n            f\"{'_'.join(features)}_{argfeat}_{aggfunc}_rolling_{rolling_aggfunc}_win_{window}\"\n        )\n        print(f'Creating feature \"{feat_name}\"')\n        return rolling_stat(\n            matrix,\n            source,\n            features,\n            feat_name,\n            window=window,\n            argfeat=argfeat,\n            aggfunc=rolling_aggfunc,\n            dtype=dtype,\n            lag_offset=lag_offset,\n        )\n    elif kind == \"expanding\":\n        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_expanding_{rolling_aggfunc}\"\n        print(f'Creating feature \"{feat_name}\"')\n        return rolling_stat(\n            matrix,\n            source,\n            features,\n            feat_name,\n            window=100,\n            argfeat=argfeat,\n            aggfunc=aggfunc,\n            dtype=dtype,\n            lag_offset=lag_offset,\n        )\n    elif kind == \"ewm\":\n        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_ewm_hl_{window}\"\n        print(f'Creating feature \"{feat_name}\"')\n        source[feat_name] = (\n            source.groupby(features)[argfeat]\n            .ewm(halflife=window, min_periods=1)\n            .agg(rolling_aggfunc)\n            .to_numpy(dtype=dtype)\n        )\n        del source[argfeat]\n        #         source = source.reset_index()\n        source[\"date_block_num\"] += 1 - lag_offset\n        return matrix.merge(source, on=[\"date_block_num\"] + features, how=\"left\")","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:43:46.355737Z","iopub.execute_input":"2022-10-10T19:43:46.356569Z","iopub.status.idle":"2022-10-10T19:43:46.375774Z","shell.execute_reply.started":"2022-10-10T19:43:46.356518Z","shell.execute_reply":"2022-10-10T19:43:46.374835Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Creating rolling mean features. \nThe combination of grouping features and window types will be selected by calling upon a large number of features and pruning them with the scikit-learn RCEFV function","metadata":{}},{"cell_type":"code","source":"matrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"artist_name_or_first_word\", \"item_category_id\", \"item_age\"],\n    window=12,\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"artist_name_or_first_word\", \"item_category_id\", \"new_item\"],\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"artist_name_or_first_word\", \"new_item\"],\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(matrix, [\"shop_id\", \"category_cluster\"], window=12)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"item_category_id\", \"item_age\"],\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix, [\"shop_id\", \"item_category_id\", \"item_age\"], window=12, reshape_source=False\n)\nmatrix = add_rolling_stats(matrix, [\"shop_id\", \"item_category_id\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"item_category_id\", \"new_item\"],\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix, [\"shop_id\", \"item_category_id\", \"new_item\"], window=12, reshape_source=False\n)\nmatrix = add_rolling_stats(matrix, [\"shop_id\"], window=12)\nmatrix = add_rolling_stats(matrix, [\"shop_id\", \"item_id\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(matrix, [\"shop_id\", \"item_id\"], window=12)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"item_name_group\", \"item_category_id\", \"new_item\"],\n    window=12,\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix, [\"shop_id\", \"item_name_group\", \"new_item\"], kind=\"expanding\", reshape_source=False\n)\nmatrix = add_rolling_stats(\n    matrix, [\"shop_id\", \"supercategory_id\", \"new_item\"], window=12, reshape_source=False\n)\n\nmatrix = add_rolling_stats(matrix, [\"shop_cluster\", \"item_id\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_cluster\", \"item_category_id\", \"item_age\"],\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix, [\"shop_cluster\", \"item_name_group\", \"new_item\"], window=12, reshape_source=False\n)\n\nmatrix = add_rolling_stats(\n    matrix, [\"category_cluster\", \"item_age\"], kind=\"expanding\", reshape_source=False\n)\nmatrix = add_rolling_stats(\n    matrix, [\"category_cluster\", \"new_item\"], kind=\"expanding\", reshape_source=False\n)\n\nmatrix = add_rolling_stats(matrix, [\"item_id\"], window=12)\n\nmatrix = add_rolling_stats(matrix, [\"artist_name_or_first_word\"], window=12)\nmatrix = add_rolling_stats(matrix, [\"artist_name_or_first_word\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(\n    matrix, [\"artist_name_or_first_word\", \"item_age\"], window=12, reshape_source=False\n)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"artist_name_or_first_word\", \"item_category_id\", \"item_age\"],\n    window=12,\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix, [\"artist_name_or_first_word\", \"new_item\"], kind=\"expanding\", reshape_source=False\n)\n\nmatrix = add_rolling_stats(\n    matrix, [\"item_category_id\", \"item_age\"], kind=\"expanding\", reshape_source=False\n)\nmatrix = add_rolling_stats(matrix, [\"item_category_id\"], window=12)\nmatrix = add_rolling_stats(matrix, [\"item_category_id\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(\n    matrix, [\"item_category_id\", \"new_item\"], kind=\"expanding\", reshape_source=False\n)\n\nmatrix = add_rolling_stats(\n    matrix, [\"item_name_group\", \"item_age\"], window=12, reshape_source=False\n)\nmatrix = add_rolling_stats(matrix, [\"item_name_group\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(matrix, [\"item_name_group\"], window=12)\n\nmatrix = add_rolling_stats(matrix, [\"platform_id\"], window=12)\nmatrix = add_rolling_stats(matrix, [\"platform_id\"], kind=\"ewm\", window=1)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T19:43:46.377801Z","iopub.execute_input":"2022-10-10T19:43:46.378599Z","iopub.status.idle":"2022-10-10T20:13:50.989305Z","shell.execute_reply.started":"2022-10-10T19:43:46.37855Z","shell.execute_reply":"2022-10-10T20:13:50.988Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()\nmatrix, oldcols = shrink_mem_new_cols(matrix, oldcols)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:13:50.991262Z","iopub.execute_input":"2022-10-10T20:13:50.991765Z","iopub.status.idle":"2022-10-10T20:14:05.85554Z","shell.execute_reply.started":"2022-10-10T20:13:50.991732Z","shell.execute_reply":"2022-10-10T20:14:05.854097Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We're gonna calculate windowed mean sales features with day resolution accuracy","metadata":{}},{"cell_type":"code","source":"matrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"item_id\"],\n    aggfunc=\"sum\",\n    rolling_aggfunc=\"sum\",\n    kind=\"rolling\",\n    window=12,\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"item_id\"],\n    aggfunc=\"sum\",\n    rolling_aggfunc=\"sum\",\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix[\"1year\"]=365\nmatrix[\"item_id_day_mean_expanding\"] = matrix[\n    \"item_id_item_cnt_month_sum_expanding_sum\"\n]/ matrix[[\"first_item_sale_days\"]].min(axis=1)\nmatrix[\"shop_id_item_id_day_mean_win_12\"] = matrix[\n    \"shop_id_item_id_item_cnt_month_sum_rolling_sum_win_12\"\n] / matrix[[\"first_item_sale_days\", \"shop_open_days\", \"1year\"]].min(axis=1)\nmatrix.loc[matrix.new_item == True, \"item_id_day_mean_expanding\",] = float(\"nan\")\nmatrix = matrix.drop(columns=[\"1year\", \"item_id_item_cnt_month_sum_expanding_sum\"])","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:14:05.856926Z","iopub.execute_input":"2022-10-10T20:14:05.857305Z","iopub.status.idle":"2022-10-10T20:16:29.763283Z","shell.execute_reply.started":"2022-10-10T20:14:05.85727Z","shell.execute_reply":"2022-10-10T20:16:29.761801Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"item_name_group\"],\n    window=12,\n    argfeat=\"item_revenue_month\",\n    dtype=\"float32\",\n)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:16:29.765001Z","iopub.execute_input":"2022-10-10T20:16:29.765394Z","iopub.status.idle":"2022-10-10T20:17:57.708799Z","shell.execute_reply.started":"2022-10-10T20:16:29.765358Z","shell.execute_reply":"2022-10-10T20:17:57.707483Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Windowed mean unique item features & ratio of new items per category with mean over previous year","metadata":{}},{"cell_type":"code","source":"matrix = add_rolling_stats(\n    matrix,\n    [\"item_category_id\"],\n    argfeat=\"new_items_cat\",\n    window=12,\n    reshape_source=True,\n    lag_offset=1,\n)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"item_name_group\"],\n    argfeat=\"name_group_new_unique_month\",\n    window=12,\n    reshape_source=True,\n    lag_offset=1,\n)\nmatrix[\"new_items_cat_1_12_ratio\"] = (\n    matrix[\"new_items_cat\"]\n    / matrix[\"item_category_id_new_items_cat_mean_rolling_mean_win_12\"]\n)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:17:57.710311Z","iopub.execute_input":"2022-10-10T20:17:57.710681Z","iopub.status.idle":"2022-10-10T20:18:49.446438Z","shell.execute_reply.started":"2022-10-10T20:17:57.710646Z","shell.execute_reply":"2022-10-10T20:18:49.445411Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()\nmatrix, oldcols = shrink_mem_new_cols(matrix, oldcols)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:18:49.447909Z","iopub.execute_input":"2022-10-10T20:18:49.448395Z","iopub.status.idle":"2022-10-10T20:18:52.068381Z","shell.execute_reply.started":"2022-10-10T20:18:49.44834Z","shell.execute_reply":"2022-10-10T20:18:52.06703Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<font size=\"+2\"><b>Lagged features and mean encodings</b></font>\n\n\nValues for the same shop-item combination from previous months","metadata":{}},{"cell_type":"code","source":"def simple_lag_feature(matrix, lag_feature, lags):\n    for lag in lags:\n        newname = lag_feature + f\"_lag_{lag}\"\n        print(f\"Adding feature {newname}\")\n        targetseries = matrix.loc[:, [\"date_block_num\", \"item_id\", \"shop_id\"] + [lag_feature]]\n        targetseries[\"date_block_num\"] += lag\n        targetseries = targetseries.rename(columns={lag_feature: newname})\n        matrix = matrix.merge(\n            targetseries, on=[\"date_block_num\", \"item_id\", \"shop_id\"], how=\"left\"\n        )\n        matrix.loc[\n            (matrix.item_age >= lag) & (matrix.shop_age >= lag) & (matrix[newname].isna()),\n            newname,\n        ] = 0\n    return matrix","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:18:52.070011Z","iopub.execute_input":"2022-10-10T20:18:52.070387Z","iopub.status.idle":"2022-10-10T20:18:52.078858Z","shell.execute_reply.started":"2022-10-10T20:18:52.070352Z","shell.execute_reply":"2022-10-10T20:18:52.077613Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matrix = simple_lag_feature(matrix, 'item_cnt_month', lags=[1,2,3])\nmatrix = simple_lag_feature(matrix, 'item_cnt_day_avg', lags=[1, 2, 3])\nmatrix = simple_lag_feature(matrix, 'item_revenue_month', lags=[1])\ngc.collect()\nprint(\"Lag features created\")\n","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:18:52.080164Z","iopub.execute_input":"2022-10-10T20:18:52.081161Z","iopub.status.idle":"2022-10-10T20:21:32.18403Z","shell.execute_reply.started":"2022-10-10T20:18:52.081097Z","shell.execute_reply":"2022-10-10T20:21:32.182598Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Lagging the mean or sum value of a target feature of each level of a categorical of features or combinations of them","metadata":{}},{"cell_type":"code","source":"def create_apply_ME(\n    matrix, grouping_fields, lags=[1], target=\"item_cnt_day_avg\", aggfunc=\"mean\"\n):\n    grouping_fields = list_if_not(grouping_fields)\n    for lag in lags:\n        newname = \"_\".join(grouping_fields + [target] + [aggfunc] + [f\"lag_{lag}\"])\n        print(f\"Adding feature {newname}\")\n        me_series = (\n            matrix.groupby([\"date_block_num\"] + grouping_fields)[target]\n            .agg(aggfunc)\n            .rename(newname)\n            .reset_index()\n        )\n        me_series[\"date_block_num\"] += lag\n        matrix = matrix.merge(me_series, on=[\"date_block_num\"] + grouping_fields, how=\"left\")\n        del me_series\n        matrix[newname] = matrix[newname].fillna(0)\n        for g in grouping_fields:\n            firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n            matrix = matrix.merge(firsts, left_on=g, right_index=True, how=\"left\")\n            matrix.loc[\n                matrix[\"date_block_num\"] < (matrix[\"firsts\"] + (lag)), newname\n            ] = float(\"nan\")\n            del matrix[\"firsts\"]\n        matrix[newname] = reduce_mem_usage(matrix[newname])\n    return matrix","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:21:32.18598Z","iopub.execute_input":"2022-10-10T20:21:32.186713Z","iopub.status.idle":"2022-10-10T20:21:32.198343Z","shell.execute_reply.started":"2022-10-10T20:21:32.186671Z","shell.execute_reply":"2022-10-10T20:21:32.19687Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matrix = create_apply_ME(matrix, [\"item_name_group\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"item_name_group\"], target=\"item_cnt_month\", aggfunc=\"sum\")\nmatrix = create_apply_ME(matrix, [\"item_id\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"item_id\"])\nmatrix = create_apply_ME(matrix, [\"platform_id\"])\nmatrix = create_apply_ME(matrix, [\"item_name_group\"])\nmatrix = create_apply_ME(matrix, [\"platform_id\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"supercategory_id\"])\nmatrix = create_apply_ME(matrix, [\"item_category_id\", \"new_item\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"shop_id\", \"item_category_id\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"shop_cluster\", \"item_id\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"shop_cluster\", \"item_id\"])\nmatrix = create_apply_ME(matrix, [\"city_code\", \"item_id\"])\nmatrix = create_apply_ME(matrix, [\"city_code\", \"item_name_group\"])","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:21:32.200668Z","iopub.execute_input":"2022-10-10T20:21:32.201678Z","iopub.status.idle":"2022-10-10T20:23:52.541862Z","shell.execute_reply.started":"2022-10-10T20:21:32.201626Z","shell.execute_reply":"2022-10-10T20:23:52.540558Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ratios between lag 1 & rolling 12 month means, to capture decreases from previous means\n","metadata":{}},{"cell_type":"code","source":"matrix[\"item_id_item_cnt_1_12_ratio\"] = (\n    matrix[\"item_id_item_cnt_month_mean_lag_1\"]\n    / matrix[\"item_id_item_cnt_month_mean_rolling_mean_win_12\"]\n)\nmatrix[\"shop_id_item_id_item_cnt_1_12_ratio\"] = (\n    matrix[\"item_cnt_day_avg_lag_1\"] / matrix[\"shop_id_item_id_day_mean_win_12\"]\n)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:23:52.543859Z","iopub.execute_input":"2022-10-10T20:23:52.54475Z","iopub.status.idle":"2022-10-10T20:23:52.580391Z","shell.execute_reply.started":"2022-10-10T20:23:52.544702Z","shell.execute_reply":"2022-10-10T20:23:52.579114Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"surplus_columns = [\n        \"item_revenue_month\",\n        \"item_cnt_day_avg\",\n        \"item_name_group\",\n        \"artist_name_or_first_word\",\n        \"item_age\",\n        \"shop_open_days\",\n        \"shop_age\",\n        \"platform_id\",\n        \"supercategory_id\",\n        \"city_code\",\n        \"category_cluster\",\n        \"shop_cluster\",\n        \"new_items_cat\",\n        \"shop_id_item_id_day_mean_win_12\",\n        \"item_id_item_cnt_month_mean_rolling_mean_win_12\",\n]\nmatrix = matrix.drop(columns=surplus_columns)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:23:52.582113Z","iopub.execute_input":"2022-10-10T20:23:52.58349Z","iopub.status.idle":"2022-10-10T20:23:56.219837Z","shell.execute_reply.started":"2022-10-10T20:23:52.583437Z","shell.execute_reply":"2022-10-10T20:23:56.218595Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)\nmatrix.to_pickle(\"matrixcheckpoint.pkl\")\nprint(\"Saved matrixcheckpoint\")\ngc.collect()\nprint(\"Mean encoding features created\")\n","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:23:56.221325Z","iopub.execute_input":"2022-10-10T20:23:56.221672Z","iopub.status.idle":"2022-10-10T20:24:08.096464Z","shell.execute_reply.started":"2022-10-10T20:23:56.221641Z","shell.execute_reply":"2022-10-10T20:24:08.095112Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can discord some of the columns that were used to generate the features.","metadata":{}},{"cell_type":"markdown","source":"Using One Hot Encoding for words in the item_name field that predict item sales. \n\nTo select k word features from the 1000's of words found in item names, words are discarded if they are not in the names of a threshold number of items, or are not in the names of new items in the test or validation months. Remaining words are then selected by the scikit-learn SelectKBest function according to their correlation with the target.","metadata":{}},{"cell_type":"code","source":"import re\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", module=\"sklearn\")\n\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\n\ndef name_token_feats(matrix, items, k=50, item_n_threshold=5, target_month_start=33):\n    def name_correction(st):\n        st = re.sub(r\"[^\\w\\s]\", \"\", st)\n        st = re.sub(r\"\\s{2,}\", \" \", st)\n        st = st.lower().strip()\n        return st\n\n    items[\"item_name_clean\"] = items[\"item_name\"].apply(name_correction)\n\n    def create_item_id_bow_matrix(items):\n        all_stopwords = stopwords.words(\"russian\")\n        all_stopwords = all_stopwords + stopwords.words(\"english\")\n\n        vectorizer = CountVectorizer(stop_words=all_stopwords)\n        X = vectorizer.fit_transform(items.loc[:, \"item_name_clean\"])\n        X = pd.DataFrame.sparse.from_spmatrix(X)\n        print(f\"{len(vectorizer.vocabulary_)} words found in all items\")\n        featuremap = {\n            col: \"word_\" + token\n            for col, token in zip(\n                range(len(vectorizer.vocabulary_)), vectorizer.get_feature_names()\n            )\n        }\n        X = X.rename(columns=featuremap)\n        return X\n\n    items_bow = create_item_id_bow_matrix(items)\n    items_bow = items_bow.clip(0, 1)  # Made the word counts binary\n    common_word_mask = items_bow.sum(axis=0) > item_n_threshold\n    target_items = matrix.query(\n        f\"date_block_num>={target_month_start} & new_item==True\"\n    ).item_id.unique()\n    target_item_mask = items_bow.loc[target_items, :].sum(axis=0) > 1\n    items_bow = items_bow.loc[:, common_word_mask & target_item_mask]\n    print(f\"{items_bow.shape[1]} words of interest\")\n    mxbow = matrix[[\"date_block_num\", \"item_id\", \"item_cnt_month\"]].query(\"date_block_num<34\")\n    mxbow = mxbow.merge(items_bow, left_on=\"item_id\", right_index=True, how=\"left\")\n    X = mxbow.drop(columns=[\"date_block_num\", \"item_id\", \"item_cnt_month\"])\n    y = mxbow[\"item_cnt_month\"].clip(0, 20)\n    selektor = SelectKBest(f_regression, k=k)\n    selektor.fit(X, y)\n    tokencols = X.columns[selektor.get_support()]\n    print(f\"{k} word features selected\")\n    return items_bow[tokencols]","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:24:08.098329Z","iopub.execute_input":"2022-10-10T20:24:08.098738Z","iopub.status.idle":"2022-10-10T20:24:08.129416Z","shell.execute_reply.started":"2022-10-10T20:24:08.098702Z","shell.execute_reply":"2022-10-10T20:24:08.127832Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"items = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nword_frame = name_token_feats(matrix, items, k=50, item_n_threshold=5)\nmatrix = matrix.merge(word_frame, left_on='item_id', right_index=True, how=\"left\")\n#Note: I used LightGBM but it didnt seem to work with sparse features\n#Note 2: Converting them to int seemed to solve it\nsparsecols = [c for c in matrix.columns if pd.api.types.is_sparse(matrix[c].dtype)]\nmatrix[sparsecols] = matrix[sparsecols].sparse.to_dense().astype('int8'\n                                                                )","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:24:08.131776Z","iopub.execute_input":"2022-10-10T20:24:08.132306Z","iopub.status.idle":"2022-10-10T20:28:10.082277Z","shell.execute_reply.started":"2022-10-10T20:24:08.132253Z","shell.execute_reply":"2022-10-10T20:28:10.081032Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I ran out of memory on kaggle's notebook so i'm saving the final feature frame and resetting the notebook's kernel","metadata":{}},{"cell_type":"code","source":"gc.collect()\nmatrix.to_pickle(\"checkpoint_final.pkl\")\nprint(\"All features generated, dataframe saved\")","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:28:10.084344Z","iopub.execute_input":"2022-10-10T20:28:10.084969Z","iopub.status.idle":"2022-10-10T20:28:25.164157Z","shell.execute_reply.started":"2022-10-10T20:28:10.084928Z","shell.execute_reply":"2022-10-10T20:28:25.162628Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%reset -f","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:28:25.168792Z","iopub.execute_input":"2022-10-10T20:28:25.169209Z","iopub.status.idle":"2022-10-10T20:28:25.496194Z","shell.execute_reply.started":"2022-10-10T20:28:25.169177Z","shell.execute_reply":"2022-10-10T20:28:25.494788Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<font size= +2>\n<b>Time for Model fitting</b></font>","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:28:25.498072Z","iopub.execute_input":"2022-10-10T20:28:25.499315Z","iopub.status.idle":"2022-10-10T20:28:25.507781Z","shell.execute_reply.started":"2022-10-10T20:28:25.499271Z","shell.execute_reply":"2022-10-10T20:28:25.506614Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matrix = pd.read_pickle(\"checkpoint_final.pkl\")\nmatrix['item_cnt_month']= matrix['item_cnt_month'].clip(0,20)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:28:25.509384Z","iopub.execute_input":"2022-10-10T20:28:25.510167Z","iopub.status.idle":"2022-10-10T20:28:30.276664Z","shell.execute_reply.started":"2022-10-10T20:28:25.510108Z","shell.execute_reply":"2022-10-10T20:28:30.275333Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create a fct that fits and returns a lightgbm regressor with or without early stopping","metadata":{}},{"cell_type":"code","source":"import warnings \nwarnings.filterwarnings(\"ignore\", module=\"lightgbm\")\n\nimport lightgbm as lgbm\n\ndef fit_booster(\n    X_train,\n    y_train,\n    X_test=None,\n    y_test=None,\n    params=None,\n    test_run=False,\n    categoricals=[],\n    dropcols=[],\n    early_stopping=True,\n):\n    if params is None:\n        params = {\"learning_rate\": 0.1, \"subsample_for_bin\": 300000, \"n_estimators\": 50}\n        \n    early_stopping_rounds = None\n    if early_stopping == True:\n        early_stopping_rounds = 30\n    \n    if test_run:\n        eval_set = [(X_train, y_train)]\n        \n    else:\n        eval_set = [(X_train, y_train), (X_test, y_test)]\n    \n    booster = lgbm.LGBMRegressor(**params)\n    \n    categoricals = [c for c in categoricals if c in X_train.columns]\n    \n    \n    booster.fit(\n        X_train,\n        y_train,\n        eval_set=eval_set,\n        eval_metric=[\"rmse\"],\n        verbose=100,\n        categorical_feature=categoricals,\n        early_stopping_rounds=early_stopping_rounds,\n        )\n    \n    return booster","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:28:30.278223Z","iopub.execute_input":"2022-10-10T20:28:30.278623Z","iopub.status.idle":"2022-10-10T20:28:32.247402Z","shell.execute_reply.started":"2022-10-10T20:28:30.278586Z","shell.execute_reply":"2022-10-10T20:28:32.244376Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Split train and validation sets from the feature matrix, month 33 used as validation set","metadata":{}},{"cell_type":"code","source":"keep_from_month = 2  # The first couple of months are dropped because of distortions to their features (e.g. wrong item age)\ntest_month = 33\ndropcols = [\n    \"shop_id\",\n    \"item_id\",\n    \"new_item\",\n]  # The features are dropped to reduce overfitting\n\nvalid = matrix.drop(columns=dropcols).loc[matrix.date_block_num == test_month, :]\ntrain = matrix.drop(columns=dropcols).loc[matrix.date_block_num < test_month, :]\ntrain = train[train.date_block_num >= keep_from_month]\nX_train = train.drop(columns=\"item_cnt_month\")\ny_train = train.item_cnt_month\nX_valid = valid.drop(columns=\"item_cnt_month\")\ny_valid = valid.item_cnt_month\ndel matrix","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:28:32.250335Z","iopub.execute_input":"2022-10-10T20:28:32.250756Z","iopub.status.idle":"2022-10-10T20:29:03.681719Z","shell.execute_reply.started":"2022-10-10T20:28:32.250716Z","shell.execute_reply":"2022-10-10T20:29:03.680465Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"These hyperparameters were found by using the hyperparameter optimization framework Optuna to optimize hyperparameters for the validation set.","metadata":{}},{"cell_type":"code","source":"params = {\n        \"num_leaves\": 966,\n        \"num_leaves\": 966,\n    \"cat_smooth\": 45.01680827234465,\n    \"min_child_samples\": 27,\n    \"min_child_weight\": 0.021144950289224463,\n    \"max_bin\": 214,\n    \"learning_rate\": 0.01,\n    \"subsample_for_bin\": 300000,\n    \"min_data_in_bin\": 7,\n    \"colsample_bytree\": 0.8,\n    \"subsample\": 0.6,\n    \"subsample_freq\": 5,\n    \"n_estimators\": 8000,\n        \n}","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:29:03.683464Z","iopub.execute_input":"2022-10-10T20:29:03.68388Z","iopub.status.idle":"2022-10-10T20:29:03.690184Z","shell.execute_reply.started":"2022-10-10T20:29:03.683835Z","shell.execute_reply":"2022-10-10T20:29:03.688671Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Fitting the booster using early stopping with validation set","metadata":{}},{"cell_type":"code","source":"categoricals = [\n        \"item_category_id\",\n        \"month\",\n] #These will be set as categorical featurs by LightGBM\n\n\nlgbooster = fit_booster(\n    X_train,\n    y_train,\n    X_valid,\n    y_valid,\n    params=params,\n    test_run=False,\n    categoricals=categoricals,\n    )","metadata":{"execution":{"iopub.status.busy":"2022-10-10T21:17:40.06609Z","iopub.execute_input":"2022-10-10T21:17:40.06671Z","iopub.status.idle":"2022-10-10T21:17:40.200318Z","shell.execute_reply.started":"2022-10-10T21:17:40.066575Z","shell.execute_reply":"2022-10-10T21:17:40.198973Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"_ = lgbm.plot_importance(lgbooster, figsize=(10,50), height=0.7, importance_type=\"gain\", max_num_features=50)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:34:00.869658Z","iopub.status.idle":"2022-10-10T20:34:00.870719Z","shell.execute_reply.started":"2022-10-10T20:34:00.870368Z","shell.execute_reply":"2022-10-10T20:34:00.870402Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n_ = joblib.dump(lgbooster, \"trained_lgbooster.pkl\")","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:34:00.872316Z","iopub.status.idle":"2022-10-10T20:34:00.873499Z","shell.execute_reply.started":"2022-10-10T20:34:00.873162Z","shell.execute_reply":"2022-10-10T20:34:00.873194Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Creating test submission now, splitting the test items from the data matrix and using the model for prediction.","metadata":{}},{"cell_type":"code","source":"matrix = pd.read_pickle(\"checkpoint_final.pkl\")\nmatrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0,20)\nkeep_from_month = 2\ntest_month = 34\ntest = matrix.loc[matrix.date_block_num==test_month, :]\nX_test = test.drop(columns=\"item_cnt_month\")\ny_test = test.item_cnt_month\ndel(matrix)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:34:00.875466Z","iopub.status.idle":"2022-10-10T20:34:00.87636Z","shell.execute_reply.started":"2022-10-10T20:34:00.876039Z","shell.execute_reply":"2022-10-10T20:34:00.87607Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test[\"item_cnt_month\"] = lgbooster.predict(X_test.drop(columns=dropcols)).clip(0, 20)\n# Merge the predictions with the provided template\ntest_orig = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\ntest = test_orig.merge(\n    X_test[[\"shop_id\", \"item_id\", \"item_cnt_month\"]],\n    on=[\"shop_id\", \"item_id\"],\n    how=\"inner\",\n    copy=True,\n)\n# Verify that the indices of the submission match the original\nassert test_orig.equals(test[[\"ID\", \"shop_id\", \"item_id\"]])\ntest[[\"ID\", \"item_cnt_month\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:34:00.877794Z","iopub.status.idle":"2022-10-10T20:34:00.878869Z","shell.execute_reply.started":"2022-10-10T20:34:00.878551Z","shell.execute_reply":"2022-10-10T20:34:00.878588Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from os import remove\nremove(\"checkpoint_final.pkl\")\nremove(\"matrixcheckpoint.pkl\")\nprint(\"Finished everything!\")","metadata":{"execution":{"iopub.status.busy":"2022-10-10T20:34:00.880388Z","iopub.status.idle":"2022-10-10T20:34:00.882015Z","shell.execute_reply.started":"2022-10-10T20:34:00.881691Z","shell.execute_reply":"2022-10-10T20:34:00.881723Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Okay, this project took me 2 months to develop, and the idea ","metadata":{}}]}